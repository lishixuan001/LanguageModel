{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>**Welcome to the Language modeling Notebook.**</h3></center>\n",
    "\n",
    "In this assignment, you are going to train a neural network to **generate news headlines**.\n",
    "To reduce computational needs, we have reduced it to headlines about technology, and a handful of Tech giants.\n",
    "In this assignment you will:\n",
    "- Learn to preprocess raw text so it can be fed into an LSTM.\n",
    "- Make use of the LSTM library of Tensorflow, to train a Language model to generate headlines\n",
    "- Use your network to generate headlines, and judge which headlines are likely or not\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a language model?**\n",
    "\n",
    "Language modeling is the task of assigning a probability to sentences in a language. Besides assigning a probability to each sequence of words, the language models also assigns a probability for the likelihood of a given word (or a sequence of words) to follow a sequence of words.\n",
    "â€” Page 105, __[Neural Network Methods in Natural Language Processing](https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984/)__, 2017.\n",
    "\n",
    "In terms of neural network, we are training a neural network to produce probabilities (classification) over a fixed vocabulary of words.\n",
    "Concretely, we are training a neural network to produce:\n",
    "$$ P ( w_{i+1} | w_1, w_2, w_3, ..., w_i), \\forall i \\in (1,n)$$\n",
    "\n",
    "** Why is language modeling important? **\n",
    "\n",
    "Language modeling is a core problem in NLP.\n",
    "\n",
    "Language models can either be used as a stand-alone to produce new text that matches the distribution of text the model is trained on, but can also be used at the front-end of a more sophisticated model to produce better results.\n",
    "\n",
    "Recently for example, the __[BERT](https://arxiv.org/abs/1810.04805)__ paper show-cased that pretraining a large neural network on a language modeling task can help improve state-of-the-art on many NLP tasks. \n",
    "\n",
    "How good can the generation of a Language model be?\n",
    "\n",
    "If you have not seen the latest post by OpenAI, you should read some of the samples they generated from their language model __[here](https://blog.openai.com/better-language-models/#sample1)__.\n",
    "Because of computational restrictions, we will not achieve as good text production, but the same algorithm is at the core. They just use more data and compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, make sure you have all these libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lishixuan/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from segtok import tokenizer\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "root_folder = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the dataset files are all in the `dataset` folder of the assignment.\n",
    "\n",
    " - If you are using this notebook locally: You should run the `download_data.sh` script.\n",
    " - If you are using the Colab version of the notebook, make sure that your Google Drive is mounted, and you verify from the file explorer in Colab that the files are viewable within `/content/gdrive/CS182_HW03/dataset/`\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 88568\n",
      "Number of validation samples: 946\n"
     ]
    }
   ],
   "source": [
    "# This cell loads the data for the model\n",
    "# Run this before working on loading any of the additional data\n",
    "\n",
    "with open(root_folder+\"dataset/headline_generation_dataset_processed.json\", \"r\") as f:\n",
    "    d_released = json.load(f)\n",
    "\n",
    "with open(root_folder+\"dataset/headline_generation_vocabulary.txt\", \"r\") as f:\n",
    "    vocabulary = f.read().split(\"\\n\")\n",
    "w2i = {w: i for i, w in enumerate(vocabulary)} # Word to index\n",
    "unkI, padI, start_index = w2i['UNK'], w2i['PAD'], w2i['<START>']\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "input_length = len(d_released[0]['numerized']) # The length of the first element in the dataset, they are all of the same length\n",
    "d_train = [d for d in d_released if d['cut'] == 'training']\n",
    "d_valid = [d for d in d_released if d['cut'] == 'validation']\n",
    "\n",
    "print(\"Number of training samples:\",len(d_train))\n",
    "print(\"Number of validation samples:\",len(d_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded the data, let's inspect one of the elements. Each sample in our dataset is has a `numerized` vector, that contains the preprocessed headline. This vector is what we will feed in to the neural network. The field `numerized` corresponds to this list of tokens. The already loaded dictionary `vocabulary` maps token lists to the actual string. Use these elements to recover `title` key of entry 1001 in the training dataset.\n",
    "\n",
    "**TODO**: Write the numerized2text function and inspect element 1001 in the training dataset (`entry = d_train[1001]`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversing the numerized: microsoft donates cloud computing ' worth $ 1 bn ' PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "From the `title` entry: Microsoft donates cloud computing 'worth $1 bn'\n"
     ]
    }
   ],
   "source": [
    "def numerized2text(numerized):\n",
    "    \"\"\" Converts an integer sequence in the vocabulary into a string corresponding to the title.\n",
    "    \n",
    "        Arguments:\n",
    "            numerized: List[int]  -- The list of vocabulary indices corresponding to the string\n",
    "        Returns:\n",
    "            title: str -- The string corresponding to the numerized input, without padding.\n",
    "    \"\"\"\n",
    "    #####\n",
    "    # BEGIN YOUR CODE HERE \n",
    "    # Recover each word from the vocabulary in the list of indices in numerized, using the vocabulary variable\n",
    "    # Hint: Use the string.join() function to reconstruct a single string\n",
    "    #####\n",
    "    \n",
    "    words = None\n",
    "    converted_string = \" \".join([vocabulary[i] for i in numerized])\n",
    "    \n",
    "    #####\n",
    "    # END YOUR CODE HERE\n",
    "    #####\n",
    "    \n",
    "    return converted_string\n",
    "\n",
    "entry = d_train[1001]\n",
    "print(\"Reversing the numerized: \"+numerized2text(entry['numerized']))\n",
    "print(\"From the `title` entry: \"+ entry['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In language modeling, we train a model to produce the next word in the sequence given all previously generated words. This has, in practice, two steps:\n",
    "\n",
    "\n",
    "    1. Adding a special <START> token to the start of the sequence for the input. This \"shifts\" the input to the right by one. We call this the \"source\" sequence\n",
    "    2. Making the network predict the original, unshifted version (we call this the \"target\" sequence)\n",
    "\n",
    "    \n",
    "Let's take an example. Say we want to train the network on the sentence: \"The cat is great.\"\n",
    "The input to the network will be \"`<START>` The cat is great.\" The target will be: \"The cat is great\".\n",
    "    \n",
    "Therefore the first prediction is to select the word \"The\" given the `<START>` token.\n",
    "The second prediction is to produce the word \"cat\" given the two tokens \"`<START>` The\".\n",
    "At each step, the network learns to predict the next word, given all previous ones.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next step is to write the build_batch function. Given a dataset, we select a random subset of samples, and will build the \"inputs\" and the \"targets\" of the batch, following the procedure we've described.\n",
    "\n",
    "**TODO**: write the build_batch function. We give you the structure, and you have to fill in where we have left things `None`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_batch(dataset, batch_size):\n",
    "    \"\"\" Builds a batch of source and target elements from the dataset.\n",
    "    \n",
    "        Arguments:\n",
    "            dataset: List[db_element] -- A list of dataset elements\n",
    "            batch_size: int -- The size of the batch that should be created\n",
    "        Returns:\n",
    "            batch_input: List[List[int]] -- List of source sequences\n",
    "            batch_target: List[List[int]] -- List of target sequences\n",
    "            batch_target_mask: List[List[int]] -- List of target batch masks\n",
    "    \"\"\"\n",
    "    \n",
    "    #####\n",
    "    # BEGIN YOUR CODE HERE \n",
    "    #####\n",
    "    \n",
    "    \n",
    "    # We get a list of indices we will choose from the dataset.\n",
    "    # The randint function uses a uniform distribution, giving equal probably to any entry\n",
    "    # for each batch\n",
    "    indices = list(np.random.randint(0, len(dataset), size=batch_size))\n",
    "    \n",
    "    # Recover what the entries for the batch are\n",
    "    batch = [dataset[i] for i in indices]\n",
    "    \n",
    "    # Get the raw numerized for this input, each element of the dataset has a 'numerized' key\n",
    "    batch_numerized = [data['numerized'] for data in batch]\n",
    "\n",
    "    # Create an array of start_index that will be concatenated at position 1 for the input.\n",
    "    # Should be of shape (batch_size, 1)\n",
    "    start_tokens = np.ones((batch_size, 1)) * start_index\n",
    "\n",
    "    # Concatenate the start_tokens with the rest of the input\n",
    "    # The np.concatenate function should be useful\n",
    "    # The output should now be [batch_size, sequence_length+1]\n",
    "    batch_input = np.concatenate((start_tokens, batch_numerized), axis=1)\n",
    "\n",
    "    # Remove the last word from each element in the batch\n",
    "    # To restore the [batch_size, sequence_length] size\n",
    "    batch_input = batch_input[:, :-1]\n",
    "    \n",
    "    # The target should be the un-shifted numerized input\n",
    "    batch_target = batch_numerized\n",
    "\n",
    "    # The target-mask is a 0 or 1 filter to note which tokens are\n",
    "    # padding or not, to give the loss, so the model doesn't get rewarded for\n",
    "    # predicting PAD tokens.\n",
    "    batch_target_mask = np.array([a['mask'] for a in batch])\n",
    "    \n",
    "    #####\n",
    "    # END YOUR CODE HERE \n",
    "    #####\n",
    "        \n",
    "    return batch_input, batch_target, batch_target_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've written the data pipelining, we are ready to write the Neural network.\n",
    "\n",
    "The steps to setting up a neural network to do Language modeling are:\n",
    "- Creating the placeholders for the model, where we can feed in our inputs and targets.\n",
    "- Creating an RNN of our choice, size, and with optional parameters\n",
    "- Using the RNN on our placeholder inputs.\n",
    "- Getting the output from the RNN, and projecting it into a vocabulary sized dimension, so that we can make word predictions.\n",
    "- Setting up the loss on the outputs so that the network learns to produce the correct words.\n",
    "- Finally, choosing an optimizer, and defining a training operation: using the optimizer to minimize the loss.\n",
    "\n",
    "We provide skeleton code for the model, you can fill in the `None` section. If you are unfamiliar with Tensorflow, we provide some idea of what functions to look for, you should use the Tensorflow online documentation.\n",
    "\n",
    "**TODO**: Replace the `None` variables with their respective code elements in the LanguageModel Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using a basic RNN/LSTM for Language modeling\n",
    "class LanguageModel():\n",
    "    def __init__(self, input_length, vocab_size, rnn_size, learning_rate=1e-4):\n",
    "        \n",
    "        # Create the placeholders for the inputs:\n",
    "        # All three placeholders should be of size [None, input_length]\n",
    "        # Where None represents a variable batch_size, and input_length is the\n",
    "        # maximal length of a sequence of words, after being padded.\n",
    "        self.input_num = tf.placeholder(tf.int32, shape=[None, input_length])\n",
    "        self.targets = tf.placeholder(tf.int32, shape=[None, input_length])\n",
    "        self.targets_mask = tf.placeholder(tf.int32, shape=[None, input_length])\n",
    "\n",
    "        # Create an embedding variable of shape [vocab_size, rnn_size]\n",
    "        # That will map each word in our vocab into a vector of rnn_size size.\n",
    "        embedding = tf.get_variable(\"embeddings\", [vocab_size, rnn_size])\n",
    "        # Use the tensorflow embedding_lookup function\n",
    "        # To embed the input_num, using the embedding variable we've created\n",
    "        input_emb = tf.nn.embedding_lookup(embedding, self.input_num)\n",
    "\n",
    "        # Create a an RNN or LSTM cell of rnn_size size.\n",
    "        # Look into the tf.nn.rnn_cell documentation\n",
    "        # You can optionally use Tensorflow Add-ons such as the MultiRNNCell, or the DropoutWrapper\n",
    "        lm_cell = tf.nn.rnn_cell.LSTMCell(rnn_size)\n",
    "        \n",
    "        # Use the dynamic_rnn function of Tensorflow to run the embedded inputs\n",
    "        # using the lm_cell you've created, and obtain the outputs of the RNN cell.\n",
    "        # You have created a cell, which represents a single block (column) of the RNN.\n",
    "        # dynamic_rnn will \"copy\" the cell for each element in your sequence, runs the input you provide through the cell,\n",
    "        # and returns the outputs and the states of the cell.\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell=lm_cell,inputs=input_emb,dtype=tf.float32)\n",
    "\n",
    "        # Use a dense layer to project the outputs of the RNN cell into the size of the\n",
    "        # vocabulary (vocab_size).\n",
    "        # output_logits should be of shape [None,input_length,vocab_size]\n",
    "        # You can look at the tf.layers.dense function\n",
    "        self.output_logits = tf.layers.dense(outputs, vocab_size)\n",
    "\n",
    "        # Setup the loss: using the sparse_softmax_cross_entropy.\n",
    "        # The logits are the output_logits we've computed.\n",
    "        # The targets are the gold labels we are trying to match\n",
    "        # Don't forget to use the targets_mask we have, so your loss is not off,\n",
    "        # And your model doesn't get rewarded for predicting PAD tokens\n",
    "        # You might have to cast the masks into float32. Look at the tf.cast function.\n",
    "        tf.cast(self.targets_mask, dtype=tf.float32)\n",
    "        self.loss = tf.losses.sparse_softmax_cross_entropy(self.targets, self.output_logits, weights=self.targets_mask)\n",
    "\n",
    "        # Setup an optimizer (SGD, RMSProp, Adam), you can find a list under tf.train.*\n",
    "        # And provide it with a start learning rate.\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)         \n",
    "\n",
    "        # We create a train_op that requires the optimizer we've created to minimize the\n",
    "        # loss we've defined.\n",
    "        # look for the optimizer.minimize function, define what should be miniminzed.\n",
    "        # You can provide it with the provide an optional global_step parameter as well that keeps of how many\n",
    "        # Optimizations steps have been run.\n",
    "        \n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        self.train_op = optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "        self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have created the Model class, we should instantiate the model. The line tf.reset_default_graph() resets the graph for the Jupyter notebook, so multiple models aren't floating around. If you have trouble with redefinition of variables, it may be worth re-running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can create our model,\n",
    "# with parameters of our choosing.\n",
    "\n",
    "tf.reset_default_graph() # This is so that when you debug, you reset the graph each time you run this, in essence, cleaning the board\n",
    "model = LanguageModel(input_length=input_length, vocab_size=vocab_size, rnn_size=256, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your objective is to train the Language on the dataset you are provided to reach a **validation loss <= 5.50**\n",
    "\n",
    "**TODO**: Train your model so that it achieves a validation loss of <= 5.5. \n",
    "\n",
    "**Careful**: we will be testing this loss on an unreleased test set, so make sure to evaluate properly on a validation set and not overfit. You must save the model you want us to test under: models/final_language_model (the .index, .meta and .data files)\n",
    "\n",
    "**Advice**:\n",
    "- It should be possible to attain loss <= 5.50 with a 1-layer LSTM of size 256 or less.\n",
    "- You should not need more than 10 epochs to attain the threshold. More passes over the data can however give you a better model.\n",
    "- You can however try using:\n",
    "    - LSTM dropout (Tensorflow has a layer for that)\n",
    "    - Multi-layer RNN cell (Tensorflow has a layer for that)\n",
    "    - Change your optimizers, tune your learning_rate, use a learning rate schedule.\n",
    "    \n",
    "**Extra credit**:\n",
    "\n",
    "Get the loss below **validation loss <= 5.00** and get 5 points of extra-credit on this assignment. Get creative,\n",
    "\n",
    "but remember, what you do should work on our held-out test set to get the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Loss:[9.210360527038574]\n",
      "[epoch 2] Loss:[9.209156036376953]\n",
      "[epoch 3] Loss:[9.2078218460083]\n",
      "[epoch 4] Loss:[9.20650577545166]\n",
      "[epoch 5] Loss:[9.203641891479492]\n",
      "[epoch 6] Loss:[9.200913429260254]\n",
      "[epoch 7] Loss:[9.197556495666504]\n",
      "[epoch 8] Loss:[9.19180679321289]\n",
      "[epoch 9] Loss:[9.182421684265137]\n",
      "[epoch 10] Loss:[9.159929275512695]\n",
      "[epoch 11] Loss:[9.136439323425293]\n",
      "[epoch 12] Loss:[9.076470375061035]\n",
      "[epoch 13] Loss:[9.025391578674316]\n",
      "[epoch 14] Loss:[8.962329864501953]\n",
      "[epoch 15] Loss:[8.713857650756836]\n",
      "[epoch 16] Loss:[8.601417541503906]\n",
      "[epoch 17] Loss:[8.448646545410156]\n",
      "[epoch 18] Loss:[8.354429244995117]\n",
      "[epoch 19] Loss:[8.248717308044434]\n",
      "[epoch 20] Loss:[8.178157806396484]\n",
      "[epoch 21] Loss:[7.951749324798584]\n",
      "[epoch 22] Loss:[7.882966995239258]\n",
      "[epoch 23] Loss:[8.004965782165527]\n",
      "[epoch 24] Loss:[7.914823055267334]\n",
      "[epoch 25] Loss:[7.982856273651123]\n",
      "[epoch 26] Loss:[8.05252456665039]\n",
      "[epoch 27] Loss:[7.890740871429443]\n",
      "[epoch 28] Loss:[7.828081130981445]\n",
      "[epoch 29] Loss:[7.73905611038208]\n",
      "[epoch 30] Loss:[7.698302268981934]\n",
      "[epoch 31] Loss:[7.750509738922119]\n",
      "[epoch 32] Loss:[7.775906085968018]\n",
      "[epoch 33] Loss:[7.591570854187012]\n",
      "[epoch 34] Loss:[7.475029945373535]\n",
      "[epoch 35] Loss:[7.526492595672607]\n",
      "[epoch 36] Loss:[7.471242427825928]\n",
      "[epoch 37] Loss:[7.448834419250488]\n",
      "[epoch 38] Loss:[7.2892165184021]\n",
      "[epoch 39] Loss:[7.719141006469727]\n",
      "[epoch 40] Loss:[7.388579368591309]\n",
      "[epoch 41] Loss:[7.609923362731934]\n",
      "[epoch 42] Loss:[7.634640216827393]\n",
      "[epoch 43] Loss:[7.685778617858887]\n",
      "[epoch 44] Loss:[7.3657708168029785]\n",
      "[epoch 45] Loss:[7.455489635467529]\n",
      "[epoch 46] Loss:[7.6926069259643555]\n",
      "[epoch 47] Loss:[7.296402454376221]\n",
      "[epoch 48] Loss:[7.329884052276611]\n",
      "[epoch 49] Loss:[7.543972015380859]\n",
      "[epoch 50] Loss:[7.583603382110596]\n",
      "[epoch 51] Loss:[7.500271797180176]\n",
      "[epoch 52] Loss:[7.365220546722412]\n",
      "[epoch 53] Loss:[7.222496032714844]\n",
      "[epoch 54] Loss:[7.287998676300049]\n",
      "[epoch 55] Loss:[7.250484466552734]\n",
      "[epoch 56] Loss:[7.2313232421875]\n",
      "[epoch 57] Loss:[7.408017635345459]\n",
      "[epoch 58] Loss:[7.308035850524902]\n",
      "[epoch 59] Loss:[7.499140739440918]\n",
      "[epoch 60] Loss:[7.023926734924316]\n",
      "[epoch 61] Loss:[7.261186122894287]\n",
      "[epoch 62] Loss:[7.3834662437438965]\n",
      "[epoch 63] Loss:[6.72848653793335]\n",
      "[epoch 64] Loss:[7.508960247039795]\n",
      "[epoch 65] Loss:[7.300468444824219]\n",
      "[epoch 66] Loss:[7.493871212005615]\n",
      "[epoch 67] Loss:[7.085367202758789]\n",
      "[epoch 68] Loss:[7.2474684715271]\n",
      "[epoch 69] Loss:[6.898455619812012]\n",
      "[epoch 70] Loss:[7.192529201507568]\n",
      "[epoch 71] Loss:[7.088857173919678]\n",
      "[epoch 72] Loss:[7.161736488342285]\n",
      "[epoch 73] Loss:[7.41424036026001]\n",
      "[epoch 74] Loss:[6.9950408935546875]\n",
      "[epoch 75] Loss:[7.257844924926758]\n",
      "[epoch 76] Loss:[6.918525695800781]\n",
      "[epoch 77] Loss:[7.113241195678711]\n",
      "[epoch 78] Loss:[7.2907915115356445]\n",
      "[epoch 79] Loss:[7.303854942321777]\n",
      "[epoch 80] Loss:[6.943804740905762]\n",
      "[epoch 81] Loss:[7.2025041580200195]\n",
      "[epoch 82] Loss:[6.870308876037598]\n",
      "[epoch 83] Loss:[6.89349889755249]\n",
      "[epoch 84] Loss:[6.82147741317749]\n",
      "[epoch 85] Loss:[7.1077375411987305]\n",
      "[epoch 86] Loss:[7.1534953117370605]\n",
      "[epoch 87] Loss:[6.858913898468018]\n",
      "[epoch 88] Loss:[6.950246810913086]\n",
      "[epoch 89] Loss:[7.047643184661865]\n",
      "[epoch 90] Loss:[7.2490434646606445]\n",
      "[epoch 91] Loss:[7.08312463760376]\n",
      "[epoch 92] Loss:[7.198662757873535]\n",
      "[epoch 93] Loss:[6.924219131469727]\n",
      "[epoch 94] Loss:[6.837902545928955]\n",
      "[epoch 95] Loss:[6.788840293884277]\n",
      "[epoch 96] Loss:[6.8409881591796875]\n",
      "[epoch 97] Loss:[7.3150553703308105]\n",
      "[epoch 98] Loss:[7.238221645355225]\n",
      "[epoch 99] Loss:[6.963853359222412]\n",
      "[epoch 100] Loss:[6.680981159210205]\n",
      "[epoch 101] Loss:[7.063756942749023]\n",
      "[epoch 102] Loss:[6.9980788230896]\n",
      "[epoch 103] Loss:[6.931678295135498]\n",
      "[epoch 104] Loss:[7.013931751251221]\n",
      "[epoch 105] Loss:[6.727647304534912]\n",
      "[epoch 106] Loss:[6.820146560668945]\n",
      "[epoch 107] Loss:[7.105581760406494]\n",
      "[epoch 108] Loss:[7.262418270111084]\n",
      "[epoch 109] Loss:[7.084790229797363]\n",
      "[epoch 110] Loss:[7.0596747398376465]\n",
      "[epoch 111] Loss:[7.195538520812988]\n",
      "[epoch 112] Loss:[7.004528999328613]\n",
      "[epoch 113] Loss:[6.853987216949463]\n",
      "[epoch 114] Loss:[6.968103885650635]\n",
      "[epoch 115] Loss:[6.740841388702393]\n",
      "[epoch 116] Loss:[7.005654811859131]\n",
      "[epoch 117] Loss:[7.032021522521973]\n",
      "[epoch 118] Loss:[6.974637031555176]\n",
      "[epoch 119] Loss:[7.302681922912598]\n",
      "[epoch 120] Loss:[7.002716064453125]\n",
      "[epoch 121] Loss:[7.137375831604004]\n",
      "[epoch 122] Loss:[7.039788722991943]\n",
      "[epoch 123] Loss:[7.044581413269043]\n",
      "[epoch 124] Loss:[7.0936598777771]\n",
      "[epoch 125] Loss:[7.084089279174805]\n",
      "[epoch 126] Loss:[6.847926616668701]\n",
      "[epoch 127] Loss:[7.024505138397217]\n",
      "[epoch 128] Loss:[7.284448623657227]\n",
      "[epoch 129] Loss:[7.048461437225342]\n",
      "[epoch 130] Loss:[7.047662258148193]\n",
      "[epoch 131] Loss:[6.876150608062744]\n",
      "[epoch 132] Loss:[7.121709823608398]\n",
      "[epoch 133] Loss:[6.956364154815674]\n",
      "[epoch 134] Loss:[6.918942451477051]\n",
      "[epoch 135] Loss:[7.0602569580078125]\n",
      "[epoch 136] Loss:[6.881109714508057]\n",
      "[epoch 137] Loss:[7.080505847930908]\n",
      "[epoch 138] Loss:[6.948446750640869]\n",
      "[epoch 139] Loss:[6.985090732574463]\n",
      "[epoch 140] Loss:[7.234247207641602]\n",
      "[epoch 141] Loss:[7.309351444244385]\n",
      "[epoch 142] Loss:[6.828507423400879]\n",
      "[epoch 143] Loss:[7.017787933349609]\n",
      "[epoch 144] Loss:[7.104522228240967]\n",
      "[epoch 145] Loss:[7.043856620788574]\n",
      "[epoch 146] Loss:[6.759156227111816]\n",
      "[epoch 147] Loss:[7.15004301071167]\n",
      "[epoch 148] Loss:[7.307005882263184]\n",
      "[epoch 149] Loss:[6.82741117477417]\n",
      "[epoch 150] Loss:[7.131899356842041]\n",
      "[epoch 151] Loss:[7.170121192932129]\n",
      "[epoch 152] Loss:[6.9655327796936035]\n",
      "[epoch 153] Loss:[6.672994613647461]\n",
      "[epoch 154] Loss:[6.832409381866455]\n",
      "[epoch 155] Loss:[7.1786370277404785]\n",
      "[epoch 156] Loss:[7.00954008102417]\n",
      "[epoch 157] Loss:[6.873937606811523]\n",
      "[epoch 158] Loss:[6.871691703796387]\n",
      "[epoch 159] Loss:[7.136232376098633]\n",
      "[epoch 160] Loss:[6.881608486175537]\n",
      "[epoch 161] Loss:[6.8848700523376465]\n",
      "[epoch 162] Loss:[6.857444763183594]\n",
      "[epoch 163] Loss:[6.946269512176514]\n",
      "[epoch 164] Loss:[6.783236026763916]\n",
      "[epoch 165] Loss:[6.9716315269470215]\n",
      "[epoch 166] Loss:[7.008728981018066]\n",
      "[epoch 167] Loss:[7.104318141937256]\n",
      "[epoch 168] Loss:[6.930367469787598]\n",
      "[epoch 169] Loss:[6.945298194885254]\n",
      "[epoch 170] Loss:[7.11298942565918]\n",
      "[epoch 171] Loss:[7.238631248474121]\n",
      "[epoch 172] Loss:[7.123811721801758]\n",
      "[epoch 173] Loss:[6.8373212814331055]\n",
      "[epoch 174] Loss:[6.929520606994629]\n",
      "[epoch 175] Loss:[6.854859352111816]\n",
      "[epoch 176] Loss:[6.92840576171875]\n",
      "[epoch 177] Loss:[7.002743244171143]\n",
      "[epoch 178] Loss:[7.070296287536621]\n",
      "[epoch 179] Loss:[6.901603698730469]\n",
      "[epoch 180] Loss:[6.958251953125]\n",
      "[epoch 181] Loss:[6.863349437713623]\n",
      "[epoch 182] Loss:[7.012360572814941]\n",
      "[epoch 183] Loss:[6.775975227355957]\n",
      "[epoch 184] Loss:[6.75049352645874]\n",
      "[epoch 185] Loss:[6.593527793884277]\n",
      "[epoch 186] Loss:[6.911382675170898]\n",
      "[epoch 187] Loss:[6.894556045532227]\n",
      "[epoch 188] Loss:[6.725137233734131]\n",
      "[epoch 189] Loss:[7.046534538269043]\n",
      "[epoch 190] Loss:[6.890835285186768]\n",
      "[epoch 191] Loss:[6.940870761871338]\n",
      "[epoch 192] Loss:[6.757989883422852]\n",
      "[epoch 193] Loss:[6.926091194152832]\n",
      "[epoch 194] Loss:[7.122257709503174]\n",
      "[epoch 195] Loss:[7.027129650115967]\n",
      "[epoch 196] Loss:[6.981756210327148]\n",
      "[epoch 197] Loss:[6.836050510406494]\n",
      "[epoch 198] Loss:[6.865170955657959]\n",
      "[epoch 199] Loss:[6.8005452156066895]\n",
      "[epoch 200] Loss:[6.993809700012207]\n",
      "[epoch 201] Loss:[6.817684650421143]\n",
      "[epoch 202] Loss:[6.890822887420654]\n",
      "[epoch 203] Loss:[6.85056209564209]\n",
      "[epoch 204] Loss:[6.926929473876953]\n",
      "[epoch 205] Loss:[6.867354393005371]\n",
      "[epoch 206] Loss:[7.061209201812744]\n",
      "[epoch 207] Loss:[6.745150566101074]\n",
      "[epoch 208] Loss:[6.858791828155518]\n",
      "[epoch 209] Loss:[7.096423625946045]\n",
      "[epoch 210] Loss:[6.973751544952393]\n",
      "[epoch 211] Loss:[7.055047512054443]\n",
      "[epoch 212] Loss:[7.033590793609619]\n",
      "[epoch 213] Loss:[6.901580333709717]\n",
      "[epoch 214] Loss:[7.033435821533203]\n",
      "[epoch 215] Loss:[6.956064701080322]\n",
      "[epoch 216] Loss:[6.900535583496094]\n",
      "[epoch 217] Loss:[7.128120422363281]\n",
      "[epoch 218] Loss:[6.697429656982422]\n",
      "[epoch 219] Loss:[6.507893085479736]\n",
      "[epoch 220] Loss:[6.720790386199951]\n",
      "[epoch 221] Loss:[6.838357925415039]\n",
      "[epoch 222] Loss:[7.03667688369751]\n",
      "[epoch 223] Loss:[6.5373969078063965]\n",
      "[epoch 224] Loss:[6.93082857131958]\n",
      "[epoch 225] Loss:[7.017391204833984]\n",
      "[epoch 226] Loss:[6.829471111297607]\n",
      "[epoch 227] Loss:[7.036257266998291]\n",
      "[epoch 228] Loss:[7.163373947143555]\n",
      "[epoch 229] Loss:[6.92613410949707]\n",
      "[epoch 230] Loss:[6.901857376098633]\n",
      "[epoch 231] Loss:[6.663180828094482]\n",
      "[epoch 232] Loss:[7.170341491699219]\n",
      "[epoch 233] Loss:[6.724380970001221]\n",
      "[epoch 234] Loss:[6.965569019317627]\n",
      "[epoch 235] Loss:[7.220995903015137]\n",
      "[epoch 236] Loss:[6.8421735763549805]\n",
      "[epoch 237] Loss:[7.077062129974365]\n",
      "[epoch 238] Loss:[6.743010520935059]\n",
      "[epoch 239] Loss:[6.869085311889648]\n",
      "[epoch 240] Loss:[6.973553657531738]\n",
      "[epoch 241] Loss:[7.002182483673096]\n",
      "[epoch 242] Loss:[6.807769775390625]\n",
      "[epoch 243] Loss:[6.9573822021484375]\n",
      "[epoch 244] Loss:[6.922129154205322]\n",
      "[epoch 245] Loss:[6.9231953620910645]\n",
      "[epoch 246] Loss:[6.729678153991699]\n",
      "[epoch 247] Loss:[6.821077346801758]\n",
      "[epoch 248] Loss:[6.910599231719971]\n",
      "[epoch 249] Loss:[6.640277862548828]\n",
      "[epoch 250] Loss:[7.004463195800781]\n",
      "[epoch 251] Loss:[6.630710124969482]\n",
      "[epoch 252] Loss:[6.710160732269287]\n",
      "[epoch 253] Loss:[6.8310956954956055]\n",
      "[epoch 254] Loss:[7.059378623962402]\n",
      "[epoch 255] Loss:[6.815213203430176]\n",
      "[epoch 256] Loss:[6.661618232727051]\n",
      "[epoch 257] Loss:[6.8877034187316895]\n",
      "[epoch 258] Loss:[6.8362345695495605]\n",
      "[epoch 259] Loss:[6.944032669067383]\n",
      "[epoch 260] Loss:[6.8257527351379395]\n",
      "[epoch 261] Loss:[6.8461408615112305]\n",
      "[epoch 262] Loss:[6.855024814605713]\n",
      "[epoch 263] Loss:[6.879830837249756]\n",
      "[epoch 264] Loss:[6.859654426574707]\n",
      "[epoch 265] Loss:[6.8032755851745605]\n",
      "[epoch 266] Loss:[6.962843894958496]\n",
      "[epoch 267] Loss:[6.914190769195557]\n",
      "[epoch 268] Loss:[6.724893093109131]\n",
      "[epoch 269] Loss:[6.704249382019043]\n",
      "[epoch 270] Loss:[6.728330612182617]\n",
      "[epoch 271] Loss:[6.982650279998779]\n",
      "[epoch 272] Loss:[6.765963077545166]\n",
      "[epoch 273] Loss:[7.236990451812744]\n",
      "[epoch 274] Loss:[6.984782695770264]\n",
      "[epoch 275] Loss:[6.759557723999023]\n",
      "[epoch 276] Loss:[7.075614929199219]\n",
      "[epoch 277] Loss:[6.9597015380859375]\n",
      "[epoch 278] Loss:[6.914700508117676]\n",
      "[epoch 279] Loss:[7.053136825561523]\n",
      "[epoch 280] Loss:[6.9215240478515625]\n",
      "[epoch 281] Loss:[7.111013889312744]\n",
      "[epoch 282] Loss:[6.81897497177124]\n",
      "[epoch 283] Loss:[6.9781975746154785]\n",
      "[epoch 284] Loss:[6.816349983215332]\n",
      "[epoch 285] Loss:[6.9781646728515625]\n",
      "[epoch 286] Loss:[6.794626712799072]\n",
      "[epoch 287] Loss:[6.724851131439209]\n",
      "[epoch 288] Loss:[6.980077743530273]\n",
      "[epoch 289] Loss:[6.9176764488220215]\n",
      "[epoch 290] Loss:[6.745274066925049]\n",
      "[epoch 291] Loss:[7.002923011779785]\n",
      "[epoch 292] Loss:[6.8040995597839355]\n",
      "[epoch 293] Loss:[6.792937278747559]\n",
      "[epoch 294] Loss:[6.769281387329102]\n",
      "[epoch 295] Loss:[6.912735462188721]\n",
      "[epoch 296] Loss:[7.053145885467529]\n",
      "[epoch 297] Loss:[6.857756614685059]\n",
      "[epoch 298] Loss:[6.763615608215332]\n",
      "[epoch 299] Loss:[6.763674259185791]\n",
      "[epoch 300] Loss:[7.040135860443115]\n",
      "[epoch 301] Loss:[6.803086280822754]\n",
      "[epoch 302] Loss:[6.808175086975098]\n",
      "[epoch 303] Loss:[6.900109767913818]\n",
      "[epoch 304] Loss:[6.884532928466797]\n",
      "[epoch 305] Loss:[6.556919574737549]\n",
      "[epoch 306] Loss:[6.655674457550049]\n",
      "[epoch 307] Loss:[6.748915195465088]\n",
      "[epoch 308] Loss:[6.9033002853393555]\n",
      "[epoch 309] Loss:[6.7503767013549805]\n",
      "[epoch 310] Loss:[6.8293137550354]\n",
      "[epoch 311] Loss:[6.767820358276367]\n",
      "[epoch 312] Loss:[7.007772445678711]\n",
      "[epoch 313] Loss:[6.889319896697998]\n",
      "[epoch 314] Loss:[6.876226902008057]\n",
      "[epoch 315] Loss:[6.670632362365723]\n",
      "[epoch 316] Loss:[7.204272270202637]\n",
      "[epoch 317] Loss:[6.650723457336426]\n",
      "[epoch 318] Loss:[6.7929582595825195]\n",
      "[epoch 319] Loss:[6.964829444885254]\n",
      "[epoch 320] Loss:[7.026834964752197]\n",
      "[epoch 321] Loss:[6.828652858734131]\n",
      "[epoch 322] Loss:[6.586176872253418]\n",
      "[epoch 323] Loss:[6.9283976554870605]\n",
      "[epoch 324] Loss:[6.698934555053711]\n",
      "[epoch 325] Loss:[6.788036346435547]\n",
      "[epoch 326] Loss:[6.747514724731445]\n",
      "[epoch 327] Loss:[6.635552406311035]\n",
      "[epoch 328] Loss:[6.9436564445495605]\n",
      "[epoch 329] Loss:[6.8066229820251465]\n",
      "[epoch 330] Loss:[6.85120153427124]\n",
      "[epoch 331] Loss:[6.6393280029296875]\n",
      "[epoch 332] Loss:[6.695468425750732]\n",
      "[epoch 333] Loss:[6.8061442375183105]\n",
      "[epoch 334] Loss:[6.86357307434082]\n",
      "[epoch 335] Loss:[7.060837268829346]\n",
      "[epoch 336] Loss:[6.642547130584717]\n",
      "[epoch 337] Loss:[6.94923210144043]\n",
      "[epoch 338] Loss:[6.916776657104492]\n",
      "[epoch 339] Loss:[6.8403801918029785]\n",
      "[epoch 340] Loss:[7.040980815887451]\n",
      "[epoch 341] Loss:[6.810572147369385]\n",
      "[epoch 342] Loss:[6.798308372497559]\n",
      "[epoch 343] Loss:[6.784782886505127]\n",
      "[epoch 344] Loss:[6.641136169433594]\n",
      "[epoch 345] Loss:[6.691824913024902]\n",
      "[epoch 346] Loss:[6.741692543029785]\n",
      "[epoch 347] Loss:[6.864394187927246]\n",
      "[epoch 348] Loss:[6.866266250610352]\n",
      "[epoch 349] Loss:[6.882460117340088]\n",
      "[epoch 350] Loss:[6.725566864013672]\n",
      "[epoch 351] Loss:[6.853515148162842]\n",
      "[epoch 352] Loss:[6.742469787597656]\n",
      "[epoch 353] Loss:[6.882564067840576]\n",
      "[epoch 354] Loss:[7.150717735290527]\n",
      "[epoch 355] Loss:[6.497088432312012]\n",
      "[epoch 356] Loss:[7.002978324890137]\n",
      "[epoch 357] Loss:[6.695054054260254]\n",
      "[epoch 358] Loss:[6.857200622558594]\n",
      "[epoch 359] Loss:[6.642979145050049]\n",
      "[epoch 360] Loss:[6.7772932052612305]\n",
      "[epoch 361] Loss:[6.939907550811768]\n",
      "[epoch 362] Loss:[6.763049125671387]\n",
      "[epoch 363] Loss:[6.81417179107666]\n",
      "[epoch 364] Loss:[6.609086513519287]\n",
      "[epoch 365] Loss:[6.917430877685547]\n",
      "[epoch 366] Loss:[6.719183444976807]\n",
      "[epoch 367] Loss:[6.754350185394287]\n",
      "[epoch 368] Loss:[6.654257297515869]\n",
      "[epoch 369] Loss:[6.619024753570557]\n",
      "[epoch 370] Loss:[6.675804615020752]\n",
      "[epoch 371] Loss:[6.595308780670166]\n",
      "[epoch 372] Loss:[6.995832443237305]\n",
      "[epoch 373] Loss:[6.852344989776611]\n",
      "[epoch 374] Loss:[6.778692722320557]\n",
      "[epoch 375] Loss:[6.943307876586914]\n",
      "[epoch 376] Loss:[6.793573379516602]\n",
      "[epoch 377] Loss:[7.0314040184021]\n",
      "[epoch 378] Loss:[6.762246608734131]\n",
      "[epoch 379] Loss:[6.632616996765137]\n",
      "[epoch 380] Loss:[6.729743957519531]\n",
      "[epoch 381] Loss:[6.543896675109863]\n",
      "[epoch 382] Loss:[6.822037696838379]\n",
      "[epoch 383] Loss:[6.614140510559082]\n",
      "[epoch 384] Loss:[6.713878154754639]\n",
      "[epoch 385] Loss:[6.781252861022949]\n",
      "[epoch 386] Loss:[6.9530510902404785]\n",
      "[epoch 387] Loss:[6.7734270095825195]\n",
      "[epoch 388] Loss:[7.1094770431518555]\n",
      "[epoch 389] Loss:[6.885622501373291]\n",
      "[epoch 390] Loss:[6.78189754486084]\n",
      "[epoch 391] Loss:[6.815333843231201]\n",
      "[epoch 392] Loss:[7.011849880218506]\n",
      "[epoch 393] Loss:[6.371397972106934]\n",
      "[epoch 394] Loss:[6.985662460327148]\n",
      "[epoch 395] Loss:[6.572556495666504]\n",
      "[epoch 396] Loss:[6.918558120727539]\n",
      "[epoch 397] Loss:[6.686178684234619]\n",
      "[epoch 398] Loss:[6.592756748199463]\n",
      "[epoch 399] Loss:[6.884135723114014]\n",
      "[epoch 400] Loss:[6.684767723083496]\n",
      "[epoch 401] Loss:[6.5264668464660645]\n",
      "[epoch 402] Loss:[6.779077529907227]\n",
      "[epoch 403] Loss:[7.0413641929626465]\n",
      "[epoch 404] Loss:[6.831579685211182]\n",
      "[epoch 405] Loss:[6.745063304901123]\n",
      "[epoch 406] Loss:[6.793038368225098]\n",
      "[epoch 407] Loss:[6.6283955574035645]\n",
      "[epoch 408] Loss:[6.833078861236572]\n",
      "[epoch 409] Loss:[6.599405765533447]\n",
      "[epoch 410] Loss:[6.685505390167236]\n",
      "[epoch 411] Loss:[6.7241740226745605]\n",
      "[epoch 412] Loss:[6.908006191253662]\n",
      "[epoch 413] Loss:[6.547293186187744]\n",
      "[epoch 414] Loss:[6.7303080558776855]\n",
      "[epoch 415] Loss:[6.84500789642334]\n",
      "[epoch 416] Loss:[6.600549221038818]\n",
      "[epoch 417] Loss:[6.921712398529053]\n",
      "[epoch 418] Loss:[6.778329372406006]\n",
      "[epoch 419] Loss:[6.692234516143799]\n",
      "[epoch 420] Loss:[6.655211925506592]\n",
      "[epoch 421] Loss:[6.824499607086182]\n",
      "[epoch 422] Loss:[6.830111026763916]\n",
      "[epoch 423] Loss:[6.795588970184326]\n",
      "[epoch 424] Loss:[6.819703578948975]\n"
     ]
    }
   ],
   "source": [
    "# Skeleton code\n",
    "# You have to write your own training process to obtain a\n",
    "# Good performing model on the validation set, and save it.\n",
    "\n",
    "experiment = root_folder+\"models/language_model_1\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Here is how you initialize weights of the model according to their\n",
    "    # Initialization parameters.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Here is how you obtain a batch:\n",
    "    num_epochs = 100000\n",
    "    batch_size = 25\n",
    "    \n",
    "    for _ in range(num_epochs):\n",
    "        batch_input, batch_target, batch_target_mask = build_batch(d_train, batch_size)\n",
    "        # Map the values to each tensor in a `feed_dict`\n",
    "        feed = { model.input_num    : batch_input, \n",
    "                 model.targets      : batch_target, \n",
    "                 model.targets_mask : batch_target_mask }\n",
    "\n",
    "        # Obtain a single value of the loss for that batch.\n",
    "        # !IMPORTANT! Don't forget to include the train_op to when using a batch from the training dataset\n",
    "        # (d_train)\n",
    "        # !MORE IMPORTANT! Don't use the train_op if you evaluate the loss on the validation set,\n",
    "        # Otherwise, your network will overfit on your validation dataset.\n",
    "\n",
    "        step, train_loss, _ = sess.run([model.global_step, model.loss, model.train_op], feed_dict=feed)\n",
    "    \n",
    "        print(\"[epoch {}] Loss:[{}]\".format(step, train_loss))\n",
    "        \n",
    "    # Here is how you save the model weights\n",
    "    # model.saver.save(sess, experiment)\n",
    "    \n",
    "    # Here is how you restore the weights previously saved\n",
    "    # model.saver.restore(sess, experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you have now trained a language model! We can now use it to evaluate likely news headlines, as well as generate our very own headlines.\n",
    "\n",
    "**TODO**: Complete the three parts below, using the model you have trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Evaluation loss\n",
    "\n",
    "To evaluate the language model, we evaluate its loss (ability to predict) on unseen data that is reserved for evaluation.\n",
    "Your first evaluation is to load the model you trained, and obtain a test loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your best performing model should go here.\n",
    "model_file = root_folder+\"models/language_model_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will evaluate your model in the model_file above\n",
    "# In a very similar way as the code below.\n",
    "# Make sure your validation loss is befow the threshold we specified\n",
    "# and that you didn't train using the validation set, as you would\n",
    "# get penalized.\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model.saver.restore(sess, model_file)\n",
    "    eval_input, eval_target, eval_target_mask = build_batch(d_valid, 500)\n",
    "    feed = {model.input_num: eval_input, model.targets: eval_target, model.targets_mask: eval_target_mask}\n",
    "    eval_loss = sess.run([model.loss], feed_dict=feed)\n",
    "    print(\"Evaluation set loss:\", eval_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Evaluation of likelihood of data\n",
    "\n",
    "One use of a language model is to see what data is more likely to have originated from the training data. Because we have trained our model on news headlines, we can see which of these headlines is more likely:\n",
    "\n",
    "``Apple to release another iPhone in September``\n",
    "\n",
    "\n",
    " ``Apple and Samsung resolve all lawsuits amicably``\n",
    " \n",
    "**TODO**: Use the model to obtain the loss the neural network assigns to each sentence.\n",
    "Because the neural network assigns probability to the words appearing in a sequence, this loss can be used as a proxy to measure how likely the sentence is to have occurred in the dataset.\n",
    "Once you have the loss for each headline, write down which sentence was judged to be more likely, and explain why/if you think this is coherent.\n",
    "\n",
    "**Your answer:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headline1 = \"Apple to release new iPhone in July\"\n",
    "headline2 = \"Apple and Samsung resolve all lawsuits\"\n",
    "\n",
    "headlines = [headline1, headline2]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model.saver.restore(sess, model_file)\n",
    "\n",
    "    for headline in headlines:\n",
    "        headline = headline.lower() # Our LSTM is trained on lower-cased headlines\n",
    "    \n",
    "        # From the code in the Preprocessing section at the end of the notebook\n",
    "        # Find out how to tokenize the headline\n",
    "        tokenized = None\n",
    "        \n",
    "        # Find out how to numerize the tokenized headline\n",
    "        numerized = None\n",
    "\n",
    "        # Learn how to pad and obtain the mask of the sequence.\n",
    "        padded, mask = None\n",
    "        \n",
    "        # Obtain the loss of the sequence, and pring it\n",
    "        \n",
    "        loss = None\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Headline:\",headline)\n",
    "        print(\"Loss of the headline:\", loss)\n",
    "\n",
    "# Important check: one headline should be more likely (and have lower loss)\n",
    "# Than the other headline. You should know which headline should have lower loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Generation of headlines\n",
    "\n",
    "We can use our language model to generate text according to the distribution of our training data.\n",
    "The way generation works is the following:\n",
    "\n",
    "We seed the model with a beginning of sequence, and obtain the distribution for the next word.\n",
    "We select the most likely word (argmax) and add it to our sequence of words.\n",
    "Now our sequence is one word longer, and we can feed it in again as an input, for the network to produce the next sentence.\n",
    "We do this a fixed number of times (up to 20 words), and obtain automatically generated headlines!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided a few headline starters that should produce interesting generated headlines.\n",
    "\n",
    "**TODO:** Get creative and find at least 2 more headline_starters that produce interesting headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    model.saver.restore(sess, model_file)\n",
    "\n",
    "    # Here are some headline starters.\n",
    "    # They're all about tech companies, because\n",
    "    # That is what is in our dataset\n",
    "    headline_starters = [\"apple has released\", \"google has released\", \"amazon\", \"tesla to\"]\n",
    "    \n",
    "    for headline_starter in headline_starters:\n",
    "        print(\"===================\")\n",
    "        print(\"Generating headline starting with: \"+headline_starter)\n",
    "\n",
    "        # Tokenize and numerize the headline. Put the numerized headline\n",
    "        # beginning in `current_build`\n",
    "        tokenized = None\n",
    "        current_build = [startI] + numerize_sequence(tokenized)\n",
    "\n",
    "        while len(current_build) < input_length:\n",
    "            # Pad the current_build into a input_length vector.\n",
    "            # We do this so that it can be processed by our LanguageModel class\n",
    "            current_padded = current_build[:input_length] + [padI] * (input_length - len(current_build))\n",
    "            current_padded = np.array([current_padded])\n",
    "\n",
    "            # Obtain the logits for the current padded sequence\n",
    "            # This involves obtaining the output_logits from our model,\n",
    "            # and not the loss like we have done so far\n",
    "            logits = None\n",
    "\n",
    "            # Obtain the row of logits that interest us, the logits for the last non-pad\n",
    "            # inputs\n",
    "            last_logits = None\n",
    "            \n",
    "            # Find the highest scoring word in the last_logits\n",
    "            # array. The np.argmax function should be useful.\n",
    "            # Append this word to our current build\n",
    "            current_build.append(None)\n",
    "        \n",
    "        # Go from the current_build of word_indices\n",
    "        # To the headline (string) produced. This should involve\n",
    "        # the vocabulary, and a string merger.\n",
    "        produced_sentence = None\n",
    "        print(produced_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All done\n",
    "\n",
    "You are done with the first part of the HW.\n",
    "\n",
    "Next notebook deals with Summarization of text!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Preprocessing (read only)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**You can skip this section, however you may find these functions useful later in the assignment**\n",
    "\n",
    "We have provided this code so you see how the dataset was generated. You will have to come back some of these functions later in the assignment, so feel free to read through, to get familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def numerize_sequence(tokenized):\n",
    "    return [w2i.get(w, unkI) for w in tokenized]\n",
    "def pad_sequence(numerized, pad_index, to_length):\n",
    "    pad = numerized[:to_length]\n",
    "    padded = pad + [pad_index] * (to_length - len(pad))\n",
    "    mask = [w != pad_index for w in padded]\n",
    "    return padded, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# You do not need to run this\n",
    "# This is to show you how the dataset was created\n",
    "# You should read to understand, so you can preprocess text\n",
    "# In the same way, in the evaluation section\n",
    "\n",
    "for a in dataset:\n",
    "    a['tokenized'] = tokenizer.word_tokenizer(a['title'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You do not need to run this\n",
    "# This is to show you how the dataset was created\n",
    "# You should read to understand, so you can preprocess text\n",
    "# In the same way, in the evaluation section\n",
    "\n",
    "word_counts = Counter()\n",
    "for a in dataset:\n",
    "    word_counts.update(a['tokenized'])\n",
    "\n",
    "print(word_counts.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# You do not need to run this\n",
    "# This is to show you how the dataset was created\n",
    "# You should read to understand, so you can preprocess text\n",
    "# In the same way, in the evaluation section\n",
    "\n",
    "# Creating the vocab\n",
    "vocab_size = 20000\n",
    "special_words = [\"<START>\", \"UNK\", \"PAD\"]\n",
    "vocabulary = special_words + [w for w, c in word_counts.most_common(vocab_size-len(special_words))]\n",
    "w2i = {w: i for i, w in enumerate(vocabulary)}\n",
    "\n",
    "# Numerizing and padding\n",
    "input_length = 20\n",
    "unkI, padI, startI = w2i['UNK'], w2i['PAD'], w2i['<START>']\n",
    "\n",
    "for a in dataset:\n",
    "    a['numerized'] = numerize_sequence(a['tokenized']) # Change words to IDs\n",
    "    a['numerized'], a['mask'] = pad_sequence(a['numerized'], padI, input_length) # Append appropriate PAD tokens\n",
    "    \n",
    "# Compute fraction of words that are UNK:\n",
    "word_counters = Counter([w for a in dataset for w in a['input'] if w != padI])\n",
    "\n",
    "print(\"Fraction of UNK words:\", float(word_counters[unkI]) / sum(word_counters.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# You do not need to run this\n",
    "# This is to show you how the dataset was created\n",
    "# You should read to understand, so you can preprocess text\n",
    "# In the same way, in the evaluation section\n",
    "\n",
    "d_released_processed   = [d for d in dataset if d['cut'] != 'testing']\n",
    "d_unreleased_processed = [d for d in dataset if d['cut'] == 'testing']\n",
    "\n",
    "with open(\"dataset/headline_generation_dataset_processed.json\", \"w\") as f:\n",
    "    json.dump(d_released_processed, f)\n",
    "\n",
    "# This file is purposefully left out of the assignment, we will use it to evaluate your model.\n",
    "with open(\"dataset/headline_generation_dataset_unreleased_processed.json\", \"w\") as f:\n",
    "    json.dump(d_unreleased_processed, f)\n",
    "    \n",
    "with open(\"dataset/headline_generation_vocabulary.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(vocabulary).encode('utf8'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
